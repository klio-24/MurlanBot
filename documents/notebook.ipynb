{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall process:\n",
    "-\tMake simple game simulation, where the bot just plays heuristically doing the lowest possible move\n",
    "-\tCreate an ML model for optimal play\n",
    "-\tMake a website that allows you to play the game with a mouse, and the bot’s moves are demonstrated too\n",
    "-\tHost the game on microservices\n",
    "-\tWrite report, cleanup github\n",
    "-\tPost linkedin\n",
    "\n",
    "To Do:\n",
    "-\tRead this https://www.pagat.com/docs/sps2172.pdf\n",
    "-\thttps://www.youtube.com/watch?v=Bj6lC93JMi0 follow this tutorial for card website\n",
    "-\tthink about how to use tensors to store data \n",
    "-\twatch a video on how the best go bot works (https://jonathan-hui.medium.com/alphago-how-it-works-technically-26ddcc085319 - read and fully understand this too) \n",
    "-\ton the website add a display that shows how many times its won and lost\n",
    "-\thttps://www.catsatcards.com/Games/Murlan.htm lay out rules in the report in a logical way\n",
    "-\thttps://www.youtube.com/watch?v=IQLkPgkLMNg&ab_channel=Wanieru \n",
    "-   http://karpathy.github.io/2019/04/25/recipe/\n",
    "-\tWould be interesting to see if the right ‘behaviours’ emerge from self-learning, e.g. putting your highest card down when you've got two cards left\n",
    "-  https://www.youtube.com/watch?v=EB-NJtNERBQ - main example for MCTS\n",
    "-  https://www.youtube.com/watch?v=62nq4Zsn8vc the guy said the way you train your policy network is to train it to give the same outputs as your MCTS???\n",
    "\n",
    "- FOR FRONT END JUST DO A COMMAND LINE OUTPUT INTERFACE - figure out how to do this simply STILL USE FLASK OR WHATEVER IT WAS CALLED\n",
    "\n",
    "To add later:\n",
    "- determine optimal card to give for second round if AI wins\n",
    "- implement policy/value network or general ML with relevant loss functions\n",
    "- from the Wanieru video, look into the Information Set card mixing bluffing thing more, and how you can exploit the behaviours of the average murlan player\n",
    "- implement quads\n",
    "- implement straights\n",
    "- implement a probability of winning at each turn just like alphago\n",
    "- skipping turn as a strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Possible moves:</b>\n",
    "- Single – 54\n",
    "- Double – 52 cards, 6 for each suit, (4C2) x 13 = 78\n",
    "- Triple – 4C3  x 13 = 52\n",
    "- Quadruple – 13\n",
    "\n",
    "total possible moves without straights:\n",
    "\n",
    "197\n",
    "\n",
    "Straight:\n",
    "5 cards to 13 cards, cant cycle to 3\n",
    "13P5*4^5 + 13P6^6 + … = 119301120\n",
    "\n",
    "TOTAL = 119301317\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "based on that 'I created an AI to play this card game because my friends dropped out video', a crude evaluation function could be total rank value of cards in hand, BUT THEN HOW DO YOU TAKE INTO ACCOUNT STRAIGHTS/TRIPLES - SEE HOW POKER DOES IT."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
